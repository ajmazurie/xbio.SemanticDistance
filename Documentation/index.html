
<h1>SemanticDistance</h1>

<p>Ontologies, or controlled vocabularies, are increasingly used in biology to describe gene functions or other concepts (e.g., illnesses, anatomy -- for a comprehensive list, see the <a href='http://www.obofoundry.org/' target='_blank'>Open Biomedical Ontologies</a> and the <a href='http://www.ebi.ac.uk/Databases/ontology.html' target='_blank'>Biological Ontology Databases</a>). A common request is to evaluate the similarity of two objects given their terms in an ontology. E.g., given two genes and their functional annotations in <a href='http://www.geneontology.org/' target='_blank'>Gene Ontology</a>, should I consider their function close or not?</p>

<p>A first crude approach to quantify this similarity is to consider the respective positions of the terms used in the ontology (seen as a directed acyclic graph of terms and their relationships). The distance between two terms can be calculated for example as the number of nodes separating them. SemanticDistance offers an implementation of a better approach proposed by Lord et al., itself based on previous works on ontologies like WordNet.</p>

<p>The idea is here to use the notion of 'information content'; we can calculate for each term in an ontology a value reflecting how informative this term is. The more the term is used, the less informative it is; the distance between two terms is then calculated by taking the information content of the parents terms they share. For example, if two terms only share the root of the ontology (the less informative of all terms), their distance is high. But if they share a rarely used parent, then their distance will be low (their similarity will be high). To obtain a complete (and better !) explanation of this algorithm you can either read the articles below or the <a href='http://oenone.net/aurelien.mazurie/data/research/completed/thesis/thesis-annexes.pdf'>annexe C</a> of my PhD thesis (in French).</p>

<p>References:</p>
<ul>
  <li>Lord P, Stevens R, Brass A, Goble C: Investigating semantic similarity measures across the Gene Ontology: the relationship between sequence and annotation. Bioinformatics 2003, 19(10):1275-83.</li>
  <li>Lin D: An Information-Theoretic Definition of Similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, Morgan Kaufmann Publishers Inc. 1998:296-304.</li>
</ul>

<h2>Change log</h2>

<ul>
  <li><b>1.1 (2009-06-08)</b> Major changes to the code:
    <ul>
      <li>Errors are now reported through Exceptions, rather than error codes</li>
      <li>The previous <code>SemanticDistanceConstructor</code> class is now <code>InformationContent</code></li>
      <li>Ontology-specific classes <code>GoHelper</code> and <code>EcHelper</code> have been removed</li>
    </ul>
  </li>
  <li><b>1.0b (2006-03-02)</b>
    <ul>
      <li>Minor changes in the errors reporting</li>
      <li>The data for EC and GO ontologies are now packaged in a separate archive</li>
    </ul>
  </li>
  <li><b>1.0 (2006-01-25)</b> Now work with any ontology; helpers for GO and EC ontologies are provided</li>
  <li><b>0.1 (2005-10-22)</b> First version</li>
</ul>

<h2>Installation</h2>

<p>The <code> semantic_distance </code> library comes as a ready-to-install package, which can be downloaded from the <a href='http://oenone.net/tools/' target='_blank'>author's website</a>. The easiest way to install it is through the <code>easy_install</code> installer, by typing <code>easy_install http://aurelien.mazurie.oenone.net/content/research/tools/semantic_distance-XX.tar.gz</code> on the command line, with XX being the version of the library. The package can also be downloaded first and later installed, again by typing <code>easy_install [path to]/semantic_distance-XX.tar.gz</code> on the command line.</p>

<p>Alternatively, the package can be downloaded, unzipped, and installed using the following command: <code>python [path to unzipped package]/setup.py install</code>.</p>

<p>Once installed, the library is called by adding <code>import semantic_distance</code> in your Python code.</p>

<h2>Usage</h2>

<p>Let us consider a small ontology of 6 terms. In order to calculate the information content of each of those terms, we need to know (1) the structure of this ontology; e.g., how terms are connected to each others, and (2) the usage of those terms; e.g., how many time each term is used to describe an object:</p>

<pre>
import semantic_distance

# Declaration of the new ontology.
# Let's say we have 6 terms organized like this (A is the root):
#
#     ,--> B (5) -->.    ,--> D (5)
#    /               \  /
#  A (3)             C (2)
#    \               /  \
#     `--> E (2) -->'    `--> F (2)
#
# (the number being the usage of each term)

# First we create a structure that contains the usage of
# each term. Terms that are not used can be excluded.
usage = {
  'A': 3,
  'B': 5,
  'C': 2,
  'D': 5,
  'E': 2,
  'F': 2
 }

# Then we create a structure declaring the direct parents of
# each term. The ONLY node that must not be present in this
# structure is the root node (which have no parent).
parents = {
  'B': ['A'],
  'C': ['B', 'E'],
  'D': ['C'],
  'E': ['A'],
  'F': ['C']
 }

# Finally, we use the InformationContent class to
# calculate the information content of each node
ic = semantic_distance.InformationContent(usage, parents)

# Then we save these values for further use
ic.to_file("my_ontology.data")
</pre>

<p>Once we have the information content of each term calculated, the <code>SemanticDistance</code> class offers several methods to calculate distance between them:</p>

<pre>
import semantic_distance

# We first reload the information content previously calculated
sd = semantic_distance.SemanticDistance.from_file("my_ontology.data")

# From now we can calculate the distance between two terms by doing
print sd.distance_between('B', 'B') # return 0.0 (the lowest distance)
print sd.distance_between('B', 'C') # return 0.419755281904
print sd.distance_between('D', 'C') # return 0.282289068435
print sd.distance_between('B', 'E') # return 1.0 (the highest distance)

# Let's imagine we have a gene annotated with 'B', 'C' and 'D'.
# We can ...

# ... get the ancestor of these three terms that have the
# best information content (i.e. the most informative one)
print sd.best_common_ancestor(['B', 'C', 'D']) # return 'B'
print sd.best_common_ancestor(['C', 'D', 'F']) # return 'C'

# ... obtain the dispersion of the annotation (average distance
# between the terms)
print sd.dispersion(['B', 'C', 'D']) # return 0.443238197585

# Now let's imagine you have two genes, one annotated with 'B',
# the other with 'C' and 'D'. To know the distance between them,
print sd.distance_between_sets(['B'], ['C', 'D']) # return 0.489060268741
</pre>
